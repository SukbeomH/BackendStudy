# BE Day29 ELK Stack

---

  **목차**

  

---

# ElasticSearch

![https://media.vlpt.us/images/hkja0111/post/5c1b319a-aa4b-4025-bf61-2901be0026ef/es.png](https://media.vlpt.us/images/hkja0111/post/5c1b319a-aa4b-4025-bf61-2901be0026ef/es.png)

**Elasticsearch**는 텍스트, 숫자, 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 **무료 오픈 소스 검색 및 분산 엔진**입니다. 전문검색엔진 (Full-text search engine)으로 개발되었으나 단순히 검색엔진을 넘어 보안, 로그분석, 전문분석 등 **다양한 영역에서 중요한 역할을 하고 있는 Elasticsearch**에 대해 알아 보겠습니다.

## Overview

**Elasticsearch는 현재는 세상에서 가장 인기가 있는 오픈소스 검색엔진**으로 수많은 개인 개발자, 기업 그리고 공공기관들로부터 사랑을 받고 있습니다. 간단한 REST API, 분산형 특징, 속도, 확장성으로 유명한 Elasticsearch는 데이터 수집, 보강, 저장, 분석, 시각화를 위한 무료 개방형 도구 모음인 **Elastic Stack의 핵심 구성 요소**입니다. 보통 **ELK Stack(Elasticsearch, Logstash, Kibana)**이라고 하는 Elastic Stack에는 이제 데이터를 Elasticsearch로 전송하기 위한 경량의 다양한 데이터 수집 에이전트인 **Beats가 포함**되어 있습니다.

# ELK Stack

![https://media.vlpt.us/images/hkja0111/post/4056319b-6f35-4c2f-9547-abe3e3fca35c/image.png](https://media.vlpt.us/images/hkja0111/post/4056319b-6f35-4c2f-9547-abe3e3fca35c/image.png)

Elasticsearch는 Apache Lucene 기반의 Java 오픈소스 분산 검색 엔진입니다. Elasticsearch를 통해 루씬 라이브러리를 단독으로 사용할 수 있게 되었으며, 방대한 양의 데이터를 신속하게, NRT(Near Real Time)으로 저장, 검색, 분석할 수 있습니다. 위에서 설명한 바와 같이 Elasticsearch는 검색을 위해 단독으로 사용되기도 하지만, ELK Stack으로 사용되기도 합니다.

단독 프로젝트인 Logstash와 Kibana와 함께 사용되며 ELK Stack으로 불려왔으나, 2013년 두 프로젝트를 정식으로 흡수하여 ELK Stack 대신 제품명을 Elastic Stack이라고 정식 명명하며 모니터링, 클라우드 서비스, 머신러닝 등의 기능을 계속해서 개발, 확장 해 나가고 있습니다.

## ELK 데이터 분석 과정

일반적인 데이터 분석 과정은 웹과 시스템 로그 데이터를 만든 뒤, 로그를 수집하고 저장 및 전처리, 분석과 시각화를 차례대로 진행합니다.

ELK스택은 **로그 집계에는 Logstash**를 **검색에는 Elasticsearch**를 **데이터를 시각화하고 분석하는 Kibana**를 사용하여 데이터 색인, 검색 및 분석을 위한 강력한 플랫폼을 제공합니다.

![https://media.vlpt.us/images/baik9261/post/42fddec6-5f1b-42aa-ba7a-5d3455476c3f/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-25%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.53.13.png](https://media.vlpt.us/images/baik9261/post/42fddec6-5f1b-42aa-ba7a-5d3455476c3f/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-25%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.53.13.png)

## Elasticsearch

Elasticsearch는 기본적으로 **모든 데이터를 색인(indexing)하여 저장하고 검색, 집계 등을 수행**하며 결과를 클라이언트 또는 다른 프로그램으로 전달하여 동작하게 합니다. 뛰어난 검색 능력과 대규모 분산 시스템을 구축하는 다양한 기능을 제공하지만 설치 과정과 사용방법은 비교적 쉽고 간편합니다.

**오픈소스**[Elasticsearch Github](https://github.com/elastic)을 통해 소스코드를 확인할 수 있고, 또한 기여도 가능합니다. Lucene이 Java로 만들어졌기에 ES도 마찬가지로 Java로 코딩되어있습니다. 따라서 반드시 Java를 설치해주어야 합니다.

**실시간 분석(NRT)**Elasticsearch의 가장 큰 특징 중 하나로, **Hadoop이라는 배치 기반 분산 시스템과 달리 ES 클러스터가 실행되고 있는 동안에는 계속해서 데이터가 입력** - 색인(indexing)되고, 동시에 **실시간에 가까운 속도(Near Real-Time)로 색인된 데이터의 검색, 집계가 가능**합니다.

**전문검색엔진 (Full-text search engine)**Lucene은 기본적으로 **역파일 색인(inverted file index)라는 구조로 데이터를 저장**하고, Lucene을 사용하고 있는 Elasticsearch도 동일한 방식으로 저장하여 가공된 텍스트를 검색합니다. 이러한 특성을 **Full-Text Search**라고 하며, Elasticsearch의 검색이 빠른 이유가 바로 Inverted Index에 있습니다.

![https://media.vlpt.us/images/hkja0111/post/08489068-bcd5-42f1-983d-1668ddccda0a/image.png](https://media.vlpt.us/images/hkja0111/post/08489068-bcd5-42f1-983d-1668ddccda0a/image.png)

이전날에 배웠던 Index 검색 기억나시죠. 다시 한번 개념을 집고 넘어가 보겠습니다. 

Index는 책에서 **맨 앞에서 볼 수 있는 목차**라고 할 수 있고, Inverted Index는 책 맨 뒤에서 **키워드마다 찾아볼 수 있도록 도와주는 찾아보기 기능**이라고 할 수 있습니다. 

Index를 사용한다면 앞에서부터 순차적으로 키워드를 찾아가겠지만, Inverted Index는 Text를 Parsing한 키워드에 대해 indexing하기 때문에 훨씬 빠른 성능을 가져옵니다.

Elasticsearch에서 Query나 반환되는 결과는 모두 JSON 형식으로 전달되기 때문에, 사전에 입력할 데이터를 JSON 형태로 가공하는 과정이 필요합니다. 물론 이 과정은 **Logstash가 변환을 지원**해줍니다.

**RESTFul API** Elasticsearch는 REST API를 기본으로 지원하며 모든 데이터 조회, 입력 삭제를 HTTP 프로토콜을 통해 Rest API로 처리합니다.

![https://media.vlpt.us/images/hkja0111/post/880fe25c-92df-405c-906a-2c2460ab5069/image.png](https://media.vlpt.us/images/hkja0111/post/880fe25c-92df-405c-906a-2c2460ab5069/image.png)

## Logstash

Logstash는 원래 Elasticsearch와 별개로 다양한 데이터 수집과 저장을 위해 개발된 프로젝트였습니다. 기존의 Elasticsearch는 데이터의 색인, 검색 기능만을 제공했으나 Logstash가 Elasticsearch의 입력수단으로 사용되기 시작하면서 통합되었습니다. Logstash가 처리하는 Data Flow는 크게 세 단계로 볼 수 있습니다.

> 입력(Inputs) ➡️  필터(Filters) ➡️  출력(Outputs)
> 

입력 기능에서 **Beats, RDBMS 등 다양한 데이터 저장소로부터 데이터를 입력** 받고 필터 기능을 통해 **데이터를 확장, 변경, 필터링 및 삭제 등의 처리를 통해 가공**을 합니다. 그 후 출력 기능을 통해 **ES, Email, Kafka와 같은 다양한 데이터 저장소로 데이터를 전송**하게 됩니다.

> Elasticsearch를 이용해 구현할 기능은 단순검색기능으로 모니터링, 머신러닝 등의 복잡한 기능은 처리하지 않기 때문에 Logstash를 활용해 이미 존재하는 RDBMS(MariaDB)의 Data를 알맞게 변환하여 ES에 제공하는 것이 핵심입니다!
> 

## Kibana

Elasticsearch는 **RESTFul하고, JSON 형식 Data로 통신하기 때문에 HTTP Protocol을 이용해 어떤 클라이언트와도 손쉽게 연동이 가능**합니다. 따라서 개발자들이 ES와 연동되는 다양한 **시각화 도구를 개발**하였고, 그 중 가장 널리 쓰이는 것이 바로 **Kibana**입니다.

Kibana는 검색과 Aggregation의 집계 기능을 이용해 Elasticsearch로 부터 문서, 집계 결과 등을 불러와 웹 도구로 시각화 및 모니터링을 합니다. **Discover, Visualize, Dashboard** 3개의 기본 메뉴와 다양한 App 들로 구성되어 있고, 플러그인을 통해 App의 설치가 가능합니다.

---

# Elasticsearch Practice

이번에는 elasticsearch를 이전에 만들어 놓았던 backend server의 `createProduct` 와 `fetchProducts` API에 연결시켜 보겠습니다. 

그전에 몇가지 알고 있어야 할 부분을 설명드리겠습니다.

## Elasticserach CRUD

![https://media.vlpt.us/images/baik9261/post/091d1653-6583-46c2-b57f-385307a95dba/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-26%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.28.03.png](https://media.vlpt.us/images/baik9261/post/091d1653-6583-46c2-b57f-385307a95dba/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-26%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.28.03.png)

Elasticserach CRUD를 사용할 때는 http 메서드를 사용합니다. 과거에는 PUT과 POST를 엄격하게 구분하는 성격이 있었지만 버전업이 되면서 엄격하게 구분하지는 않고 있습니다.

## RDB & Elasticserach

![https://media.vlpt.us/images/baik9261/post/90b1d9c3-3a9e-4976-b87a-fc302e39b347/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-26%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.31.49.png](https://media.vlpt.us/images/baik9261/post/90b1d9c3-3a9e-4976-b87a-fc302e39b347/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-26%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.31.49.png)

- 인덱스(Index), 타입(Type), 도큐먼트(Document)의 단위를 갖습니다.
- 도큐먼트는 엘라스틱서치의 데이터가 저장되는 최소 단위입니다.
- 여러 개의 도큐먼트는 하나의 타입입니다.
- 다시 여러 개의 타입은 하나의 인덱스로 구성됩니다.

## Query DSL

Elasticsearch는 쿼리를 실행하는 데 사용할 수 있는 JSON 스타일 도메인 관련 언어를 제공하는데 이것이 QueryDSL입니다. Query DSL은 HQL(Hibernate Query Language) 쿼리를 타입에 안전하게 생성 및 관리할 수 있게 해주는 프레임워크 입니다. 

즉 QueryDSL은 자바 코드 기반으로 쿼리를 작성하게 해줍니다. 몇 가지 기본 예제를 통해 기본 문법을 익히고 Nest backend server에 적용해보겠습니다.

### _bulk

데이터를 한번에 생성하기 위해 `_bulk`를 사용할 수 있습니다. 

[사용 예시](https://esbook.kimjmin.net/04-data/4.3-_bulk)는 아래와 같습니다.

![Untitled](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/Untitled.png)

예제로 사용할 데이터를 생성하겠습니다.

```
POST tourcompany/_bulk
{"index" : {"_id" : "1"}}
{"name" : "Alfred","phone" : "010-1234-5678","holyday_dest" : "Disneyland","departure_date":"2017/01/20"}
{"index" : {"_id" : "2"}}
{"name" : "Huey","phone" : "010-2222-2222","holyday_dest" : "Disneyland","departure_date" : "2017/01/20"}
{"index" : {"_id" : "3"}}
{"name" : "Naomi","phone" : "010-3333-3333","holyday_dest" : "Hawai","departure_date" : "2017/01/10"}
{"index" : {"_id" : "4"}}
{"name" : "Andra","phone" : "010-6666-7777","holyday_dest" : "BoraBora","departure_date" : "2017/01/11"}
{"index" : {"_id" : "5"}}
{"name" : "Paul","phone" : "010-9999-8888","holyday_dest" : "Hawai","departure_date" : "2017/01/10"}
{"index" : {"_id" : "6"}}
{"name" : "Clin","phone" : "010-5555-4444","holyday_dest" : "Venice","departure_date" : "2017/01/16"}
```

### _search

문서 검색을 위해 `_search`를 사용합니다. 

[4.4 검색 API - _search API](https://esbook.kimjmin.net/04-data/4.4-_search)

### from & size

```
POST /tourcompany/_search
{
  "query": { "match_all": {} },
  "from": 1,
  "size": 2
}
```

예제에서의 from은 결과의 1번째 항목부터 보여주며, Default는 1입니다. 

size는 결과를 2 개만 불러오며, Default는 10입니다.

### sort

```
POST /tourcompany/_search
{
"query": { "match_all": {} },
"sort": { "_id": "desc" }
}
```

match_all을 수행하여 모든 데이터를 조회하면 이 결과를 _id 기준으로 내림차순으로 정렬해서 반환합니다.

### source

```
POST /tourcompany/_search
{
  "query": { "match_all": {} },
  "_source": ["name"]
}
```

전체 소스 문서가 반환되기를 원하지 않으면 _source 내의 일부 필드만 지정하여 결과를 반환할 수 있습니다.

### match & match_pharse

```
POST /tourcompany/_search
{
  "query": { "match": { "name": "Paul" } }
}
```

**name**에 **Paul**이 포함되어있는 모든 문서를 검색합니다. 

```
POST /tourcompany/_search
{
  "query": { "match_phrase": { "name": "Paul" } }
}
```

위의 예에서 `match`와 `match_pharse`의 결과는 동일합니다. 

하지만 만약 **{ "name": "mile stone" }**을 검색하는것이었다면, match는 **mile** 또는 **stone**과 일치하면 반환하지만 match_pharse는 “mile stone”이라는 문구와 정확히 일치하면 반환합니다.

### must

> 여러 쿼리를 조합해, 쿼리가 참인 도큐먼트들을 검색합니다.
> 

```
POST /tourcompany/_search
{
  "query":{
    "bool": {
       "must": [
             { "match": { "name": "Paul" } },
             { "match": { "phone": "010-9999-8888" } }
      ]
    }
  }
}
```

두 개의 일치 쿼리를 작성하고 name에 "Paul" phone에 "010-9999-8888"을 포함하는 모든 결과를 반환합니다.

```
POST /tourcompany/_search
{
  "query":{
    "bool": {
       "must": [{ "match": { "name": "Paul" } }],
        "must_not": [{"match": {"holyday_dest": "Venice"}}]
    }
  }
}
```

must에 일치하는 쿼리를 작성하고 must_not에는 일치하지 않을 쿼리를 작성합니다. 

### prefix Query

```
POST /tourcompany/_search
{
  "query": {
    "prefix": {
      "holyday_dest": {
        "value": "di"
      }
    }
  }
}
```

Prefix Query는 Elastic에서 제공하는 앞 글자 일치 검색 기능입니다. 

예제는 앞 글자 기준으로 holyday_dest에 "di"가 포함되어 있는 결과를 반환합니다. 

## **ElasticSearch Connect With NestJS**

**class** 폴더 안에 `29-01-elasticsearch-example` 폴더를 만들어 주세요.

`29-01-elasticsearch-example` 폴더 안에 `28-04-redis` 폴더 안에 있는 모든 파일을 복사 붙여넣기 해주세요.

먼저 필요한 모듈을 설치해 주세요.

`$yarn add @nestjs/elasticsearch`

![module.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/module.png)

검색 로직이 구현될 수 있는 module에 elasicsearch의 속성을 적용시켜 주세요. **우리는 상품에 대해 검색을 진행할 계획이기 때문에 product.module.ts에 연결시켜주었습니다.**

## Connecte With createProduct & fetchProduct

![service.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/service.png)

@nestjs/elasticsearch에서 제공하는 ElasticSearch를 생성자 주입을 해주었습니다.

fetchProduct에서 QueryDSL을 사용해 search 메 서드로 elasticsearch에서 조회 연습을 했습니다.

createProduct에서 QueryDSL을 사용해 create 메 서드로 elasticsearch에서 생성 연습을 했습니다.

## Integration With Elasticsearch Docker

```yaml
version: '3.3'

services:
  my_backend:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - ./.env
    volumes:
      - ./src:/my_backend/src
    ports:
      - 3000:3000

  my_database:
    platform: linux/x86_64
    image: mysql:latest
    environment:
      MYSQL_DATABASE: 'myproject'
      MYSQL_ROOT_PASSWORD: 'root'
    ports:
      - 3306:3306

  my_redis:
    image: redis:latest
    ports:
      - 6379:6379

  elasticsearch:
    image: elasticsearch:7.17.0
    environment:
      discovery.type: single-node
    ports:
      - 9200:9200
```

docker-compose.yaml 파일을 다음과 같이 정의했습니다. elasticsearch는 7버전을 선택해 주세요. 8버전 부수적인 설정값들이 많아 간단하게 7버전을 선택을 추천드립니다.

elasticsearch의 default port는 9200입니다.

`docker-compose build`

`docker-compose up`

을 입력해 image를 build 하고 container를 실행시켜주세요.

![스크린샷 2022-03-02 오전 11.58.24.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.58.24.png)

![스크린샷 2022-03-02 오후 12.01.54.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.01.54.png)

[http://localhost:9200/](http://localhost:9200/)에 정상적으로 접속이 된다면 elasticsearch는 정상적으로 실행된 겁니다.

![스크린샷 2022-03-02 오후 12.06.08.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.06.08.png)

`localhost:3000/graphql`에 들어가서 createProduct를 요청합니다. 

입력받은 정보로 elasticsearch에 index를 생성하고 데이터가 저장됩니다. 

![스크린샷 2022-03-02 오후 12.08.03.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.08.03.png)

![스크린샷 2022-03-02 오후 12.07.47.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.07.47.png)

fetchProducts에 요청을 보내서 elasticserach에 방금전 생성한 index가 검색되었습니다. 

---

# ****Synchronize Logstash & Mysql(Data Polling)****

****Logstash****는 엘라스틱 스택에서 일종의 **ETL(Extract, Transform an Load)의 역할을 담당하는 컴포넌트** 입니다. 

크게 **인풋(Input), 필터(Filter), 아웃풋(Output)**의 세 가지 플러그인 구조로 되어 있어서 데이터 처리를 위한 파이프라인을 계속 연결해서 처리한 다음에 최종적으로 아웃풋을 엘라스틱 서치로 지정해서 인덱싱하는 구조입니다.

이렇게 데이터베이스에서 데이터를 뽑아와서 넣어주는 방식을 **Data Polling**이라고 합니다.

![https://media.vlpt.us/images/hkja0111/post/24c426ca-f886-4743-bbdb-974ff755100b/image.png](https://media.vlpt.us/images/hkja0111/post/24c426ca-f886-4743-bbdb-974ff755100b/image.png)

## **Start Logstash**

**class** 폴더 안에 `29-02-logstash-polling-mysql` 폴더를 만들어 주세요.

`29-02-logstash-polling-mysql` 폴더 안에 `29-01-elasticsearch-example` 폴더 안에 있는 모든 파일을 복사 붙여넣기 해주세요.

`29-02-logstash-polling-mysql` 디렉토리에 elk폴더를 만들고 elk안에 logstash폴더를 만들어주세요.

![log.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/log.png)

Logstash를 기동하기 전에는 `conf` 파일을 생성하여 **Input, Filter, Output** 설정을 해줘야 합니다. 

- **Inputs** : 모든 형태, 크기, 소스의 데이터 수집한다. 형태가 매우 자유롭게 때문에 대부분의 데이터를 가져올 수 있습니다.
- **Filters** : 데이터 이동 과정에서의 구문 분석 및 변환 처리를 합니다.
- **Outputs** : 스태시를 선택하여 데이터 전송합니다.

### Inputs

- file : UNIX 명령 tail -0F와 매우 비슷하게 파일 시스템의 파일에서 읽음
- syslog : RFC3164 형식에 따라 syslog 메시지 및 구문 분석을 위해 잘 알려진 포트 514를 수신
- redis : redis 채널과 redis 목록을 모두 사용하여 redis 서버에서 읽음
- beat : Filebeat에서 보낸 이벤트를 처리합니다.

### Filters

- grok : 임의의 텍스트를 구성. 임의의 텍스트를 구성 현재 구조화되지 않은 로그 데이터를 구문 분석
- mutate : 이벤트 필드에서 일반적인 변환을 수행합니다. 이벤트의 이 및 데이터 수정 및 제거
- drop : 이벤트를 완전히 삭제
- clone : 이벤트를 복사
- geoip : IP 주소의 지리적 위치에 대한 정보를 추가 (Kibana의 지도 차트로 사용)

### Outputs

- elasticsearch : Elasticsearch에 데이터 전송. 데이터를 효율적이고 편리하며 쉽게 쿼리 형식으로 저장
- file : 이벤트 데이터를 디스크의 파일로 저장
- graphite : 이벤트 데이터를 Graphite에 전송. 이 데이터는 통계를 저장하고 그래프로 나타내기 위한 널리 사용되는 오픈 소스 도구(http://graphite.readthedocs.io/ko/latest/)
- statsd : statsd에 이벤트 데이터를 전송. “카운터 및 타이머와 같은 통계를 수신하고 UDP를 통해 전송되며 하나 이상의 플러그 가능한 백엔드 서비스의 집계를 보내는” 서비스

![conffile.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/conffile.png)

`29-02-logstash-polling-mysql` ⇒ `elk` ⇒ `logstash` 에 logstash.conf 파일을 만들고 다음 내용으로 수정해주세요.

**Input에는 Mysql에 대한 설정이 들어갑니다.**

- jdbc_driver_library : Mysql Connector jar 설치 경로
- jdbc_driver_class : mysql 드라이버 클래스
- jdbc_connection_string : Mysql에서 가져올 DB
- schedule : Sql Query 실행하는 주기. [cron expresstions](https://docs.oracle.com/cd/E12058_01/doc/doc.1014/e12030/cron_expressions.htm)로 설정합니다. “`* * * * *`”는 매 분마다 실행되도록 설정한 것입니다. schedule을 설정하지 않으면 한 번만 실행됩니다.
- statement: 직접 Sql 문을 지정하여 원하는 칼럼만 가져옵니다. SQL 문을 파일을 읽어 실행할 수도 있습니다.

**Output에는 연결해 줄 Elastic Search에 대한 설정이 들어갑니다.**

- host: Elasticsearch의 호스트 주소를 설정
- index: 생성할 index 명 설정
- document_id: RDBMS의 Row에 해당됩니다. Primary Key를 지정해 줍니다..

### JDBC Input Plugin

JDBC Input Plugin은 Logstash의 많은 **내장 인풋 플로그인중 하나**입니다. Mysql처럼 기본적으로 JDBC Interface를 지원하는 모든 데이터베이스는 이 인풋 플러그인을 통해 Elasticsearch로 Indexing할 수 있습니다. 개념적으로 Logstash의 JDBC 인풋 플러그인은 주기적으로 MySQL을 폴링하는 루프를 실행합니다.

> Logtstash는 크론 문법을 통해 주기적으로 데이터를 가져올 수 있고, 한 번만 실행시켜 데이터를 로딩하도록 설정할 수도 있습니다. Database에 원하는 JDBC Query를 던지고, Return한 결과에서 각 Row가 하나의 이벤트로 맵핑되며 Column이 Elasticsearch의 Field로 대응되는 형태로 Indexing합니다. 기본적인 Data Flow는 아래와 같습니다.
> 

![https://media.vlpt.us/images/hkja0111/post/c1456d6a-6fd9-4575-9172-e282e63f7059/image.png](https://media.vlpt.us/images/hkja0111/post/c1456d6a-6fd9-4575-9172-e282e63f7059/image.png)

[MySQL :: Download MySQL Connector/J (Archived Versions)](https://downloads.mysql.com/archives/c-j/)

직접 mysql 사이트에 들어가서 설치해도 되지만 버전을 맞추기 위해 파일 형식으로 공유하겠습니다.

[mysql-connector-java-8.0.28.jar](https://drive.google.com/file/d/1kc1ZrwqPInenE8nmiWMMEhu66Uvckh3M/view?usp=sharing)

`mysql-connector-java-8.0.28.jar`파일을 다운 받아서 `29-02-logstash-polling-mysql` ⇒ `elk` ⇒ `logstash` 폴더에 넣어주세요.

```yaml
version: '3.3'

services:
  my_backend:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - ./.env
    volumes:
      - ./src:/my_backend/src
    ports:
      - 3000:3000

  my_database:
    platform: linux/x86_64
    image: mysql:latest
    environment:
      MYSQL_DATABASE: 'myproject'
      MYSQL_ROOT_PASSWORD: 'root'
    ports:
      - 3306:3306

  my_redis:
    image: redis:latest
    ports:
      - 6379:6379

  elasticsearch:
    image: elasticsearch:7.17.0
    environment:
      discovery.type: single-node
    ports:
      - 9200:9200

  logstash:
    image: logstash:7.17.0
    volumes:
      - ./elk/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./elk/logstash/mysql-connector-java-8.0.28.jar:/usr/share/logstash/mysql-connector-java-8.0.28.jar
```

`docker-compose.yaml` 파일을 다음과 같이 정의했습니다. logstash도 7버전을 선택해 주세요. 

컨테이너 환경에서 설치한 logstash의 conf 파일을 프로젝트에 생성했던 `logstash.conf`로 읽혀주었습니다. `mysql-connector-java-8.0.28.jar`파일 또한 마찬가지 입니다.

logstash의 default port는 9600입니다.

`docker-compose build`

`docker-compose up`

을 입력해 image를 build 하고 container를 실행시켜주세요.

![스크린샷 2022-03-02 오후 4.48.04.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.48.04.png)

![스크린샷 2022-03-02 오후 4.07.15.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.07.15.png)

dbeaver를 실행시켜 container로 띄워진 데이터 베이스와 연결해서 myproject에 product 테이블에 procedure를 사용해서 더미 데이터를 생성했습니다.

![catindi.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/catindi.png)

[http://localhost:9200/_cat/indices](http://localhost:9200/_cat/indices) 에 들어가면 **Logstash를 사용해 Mysql의 Data를 Elasticsearch로 변환하여 입력**하는 것을 확인할 수 있습니다. 아직 yellow이지만 시간이 지나면 green으로 변할 겁니다.

![스크린샷 2022-03-02 오후 4.57.26.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.57.26.png)

logstash의 로그를 확인해 보면 1분에 한번식 mysql에 쿼리를 날려 product 테이블을 조회하는 걸 확인할 수 있습니다.

![스크린샷 2022-03-02 오후 12.08.03.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.08.03.png)

![스크린샷 2022-03-02 오후 5.06.25.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.06.25.png)

[http://localhost:3000/graphql](http://localhost:3000/graphql) fetchProducts에 요청을 보내서 index가 검색이 됩니다.

## Data Polling With Time

**class** 폴더 안에 `29-03-logstash-polling-mysql-with-time` 폴더를 만들어 주세요.

`29-03-logstash-polling-mysql-with-time` 폴더 안에 `29-02-logstash-polling-mysql` 폴더 안에 있는 모든 파일을 복사 붙여넣기 해주세요.

이전에는 `*` 를 사용해서 모든 컬럼의 데이터를 뽑아왔는데 이번에는 원하는 컬럼의 데이터를 뽑아 올 것입니다.

```
input {
    jdbc {
        jdbc_driver_library => "/usr/share/logstash/mysql-connector-java-8.0.28.jar"
        jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
        jdbc_connection_string => "jdbc:mysql://my_database:3306/myproject"
        jdbc_user => "root"
        jdbc_password => "root"
        schedule => "* * * * *"
        use_column_value => true
        tracking_column => "updatedat"
        last_run_metadata_path => "./aaa.txt"
        tracking_column_type => "timestamp" 
        statement => "select id, name, price, updatedat from product where updatedat > :sql_last_value order by updatedat asc"
    }
}

output {
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "myproduct2"
    }
}
```

`statement`에서 실행할 쿼리는 상품의 id, name, price, updatedat 칼럼에 대한 **데이터를 생성된 시간의 오름차순으로 정렬해서 뽑아올 것입니다.**

`last_run_metadata_path`에는 마지막 뽑아온 데이터를 텍스트로 저장합니다.

`logstash.conf` 파일을 다음과 같이 수정해 주세요.

`docker-compose build`

`docker-compose up`

을 입력해 image를 build 하고 container를 실행시켜주세요.

![스크린샷 2022-03-02 오후 4.48.04.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.48.04.png)

![스크린샷 2022-03-02 오후 4.07.15.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.07.15.png)

`dbeaver`를 실행시켜 container로 띄워진 데이터 베이스와 연결해서 myproject에 product 테이블에 procedure를 사용해서 더미 데이터를 생성했습니다.

![스크린샷 2022-03-02 오후 5.35.48.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.35.48.png)

[http://localhost:9200/_cat/indices](http://localhost:9200/_cat/indices) 에 들어가면 **Logstash를 사용해 Mysql의 Data를 Elasticsearch로 변환하여 입력**하는 것을 확인할 수 있습니다. 

![스크린샷 2022-03-02 오후 12.08.03.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.08.03.png)

![Untitled](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/Untitled%201.png)

[http://localhost:3000/graphql](http://localhost:3000/graphql) fetchProducts에 요청을 보내서 index가 검색이 됩니다. 그런데 **updatedat의 소수점이 잘리는 현상 발생합니다.**

```
input {
    jdbc {
        jdbc_driver_library => "/usr/share/logstash/mysql-connector-java-8.0.28.jar"
        jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
        jdbc_connection_string => "jdbc:mysql://my_database:3306/myproject"
        jdbc_user => "root"
        jdbc_password => "root"
        schedule => "* * * * *"
        use_column_value => true
        tracking_column => "updatedat"
        last_run_metadata_path => "./aaa.txt"
        tracking_column_type => "numeric"
        statement => "select id, name, price, unix_timestamp(updatedat) as updatedat from product where unix_timestamp(updatedat) > :sql_last_value order by updatedat asc"
    }
}

output {
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "myproduct3"
    }
}
```

이전에 tracking_column_type를 timestamp였는데 numeric으로 변경해주세요.

![스크린샷 2022-03-02 오후 5.52.11.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.52.11.png)

[http://localhost:9200/_cat/indices](http://localhost:9200/_cat/indices) 에 들어가면 **Logstash를 사용해 Mysql의 Data를 Elasticsearch로 변환하여 입력**하는 것을 확인할 수 있습니다. 

![스크린샷 2022-03-02 오후 12.08.03.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.08.03.png)

![스크린샷 2022-03-02 오후 5.53.16.png](BE%20Day29%20ELK%20Stack%2079a49c9f6bb145169017d15edb28959a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.53.16.png)

[http://localhost:3000/graphql](http://localhost:3000/graphql) fetchProducts에 요청을 보내서 index가 검색이 됩니다. **updateat의 소수점이 잘리지 않습니다.**